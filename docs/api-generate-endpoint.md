# é€šç”¨LLMç”Ÿæˆæ¥å£æ–‡æ¡£

## ğŸ“Œ æ¦‚è¿°

æ–°å¢çš„ `/api/generate` ç«¯ç‚¹æä¾›äº†**ç›´æ¥è®¿é—®åº•å±‚LLMçš„èƒ½åŠ›**ï¼Œä¸ç»è¿‡ZenAIçš„ç¦…æ„åŒ…è£…ï¼Œé€‚ç”¨äºéœ€è¦è‡ªå®šä¹‰promptçš„åº”ç”¨åœºæ™¯ã€‚

## ğŸ”— ç«¯ç‚¹ä¿¡æ¯

### **URL**
```
POST /api/generate
```

### **ç”¨é€”**
- ç¬¦å·ç‚¼é‡‘æœ¯æ¸¸æˆ
- è‡ªå®šä¹‰AIåº”ç”¨
- éœ€è¦å®Œå…¨æ§åˆ¶promptçš„åœºæ™¯

## ğŸ“ è¯·æ±‚æ ¼å¼

### **Request Body** (JSON)

```json
{
  "prompt": "ä½ çš„å®Œæ•´æç¤ºè¯",
  "temperature": 0.7,
  "max_tokens": 500
}
```

### **å‚æ•°è¯´æ˜**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | èŒƒå›´ | è¯´æ˜ |
|------|------|------|--------|------|------|
| `prompt` | string | âœ… | - | 1-50000 chars | å‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯ |
| `temperature` | float | âŒ | 0.7 | 0.0-2.0 | é‡‡æ ·æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰ |
| `max_tokens` | int | âŒ | 1000 | 1-4000 | æœ€å¤§ç”Ÿæˆtokenæ•° |

## ğŸ“¤ å“åº”æ ¼å¼

### **Response Body** (JSON)

```json
{
  "text": "LLMç”Ÿæˆçš„æ–‡æœ¬",
  "timestamp": "2026-01-20T12:34:56.789Z"
}
```

### **å­—æ®µè¯´æ˜**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `text` | string | LLMç”Ÿæˆçš„åŸå§‹æ–‡æœ¬ |
| `timestamp` | datetime | ç”Ÿæˆæ—¶é—´æˆ³ |

## ğŸ’» ä½¿ç”¨ç¤ºä¾‹

### **1. JavaScript (Fetch API)**

```javascript
async function generateWithLLM(prompt) {
  const response = await fetch('/api/generate', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      prompt: prompt,
      temperature: 0.7,
      max_tokens: 500,
    }),
  });
  
  if (!response.ok) {
    throw new Error(`API request failed: ${response.status}`);
  }
  
  const data = await response.json();
  return data.text;
}

// ä½¿ç”¨ç¤ºä¾‹
const result = await generateWithLLM('è¯·ç”Ÿæˆä¸€ä¸ªæ•°å­¦å‡½æ•°...');
console.log(result);
```

### **2. Python (requests)**

```python
import requests

def generate_with_llm(prompt, temperature=0.7, max_tokens=500):
    response = requests.post('http://localhost:8000/api/generate', json={
        'prompt': prompt,
        'temperature': temperature,
        'max_tokens': max_tokens,
    })
    response.raise_for_status()
    return response.json()['text']

# ä½¿ç”¨ç¤ºä¾‹
result = generate_with_llm('è¯·ç”Ÿæˆä¸€ä¸ªæ•°å­¦å‡½æ•°...')
print(result)
```

### **3. cURL**

```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "è¯·ç”Ÿæˆä¸€ä¸ªæ•°å­¦å‡½æ•°...",
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

## ğŸ® ç¬¦å·ç‚¼é‡‘æœ¯æ¸¸æˆé›†æˆ

### **ä¿®æ”¹å‰** (ä½¿ç”¨ZenAI `/api/chat`)

```javascript
// âŒ æ—§æ–¹å¼ - ä¼šç»è¿‡ZenAIçš„ç¦…æ„åŒ…è£…
const response = await fetch('/api/chat', {
  method: 'POST',
  body: JSON.stringify({
    user_input: prompt,
    language: 'zh',
  }),
});
const data = await response.json();
return data.response_text; // å¯èƒ½æ˜¯ç¦…æ„å›ç­”ï¼Œä¸æ˜¯JSON
```

### **ä¿®æ”¹å** (ä½¿ç”¨é€šç”¨ `/api/generate`)

```javascript
// âœ… æ–°æ–¹å¼ - ç›´æ¥è°ƒç”¨LLM
const response = await fetch('/api/generate', {
  method: 'POST',
  body: JSON.stringify({
    prompt: prompt,
    temperature: 0.7,
    max_tokens: 500,
  }),
});
const data = await response.json();
return data.text; // åŸå§‹LLMè¾“å‡º
```

## âš™ï¸ Temperature å‚æ•°æŒ‡å—

| Temperature | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|-------------|------|----------|
| 0.0 - 0.3 | æœ€ç¡®å®šæ€§ï¼Œé‡å¤æ€§é«˜ | éœ€è¦ç²¾ç¡®ã€ä¸€è‡´çš„è¾“å‡º |
| 0.4 - 0.7 | **å¹³è¡¡** | å¤§å¤šæ•°åº”ç”¨ï¼ˆæ¨èï¼‰ |
| 0.8 - 1.2 | æ›´å¤šåˆ›æ„å’Œå˜åŒ– | åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´ |
| 1.3 - 2.0 | é«˜åº¦éšæœº | è‰ºæœ¯åˆ›ä½œã€å®éªŒæ€§åº”ç”¨ |

## ğŸ”’ å®‰å…¨æ€§è€ƒè™‘

1. **è¾“å…¥éªŒè¯**: prompté•¿åº¦é™åˆ¶åœ¨50000å­—ç¬¦
2. **è¾“å‡ºé™åˆ¶**: max_tokensæœ€å¤§4000
3. **é”™è¯¯å¤„ç†**: å¤±è´¥æ—¶è¿”å›500çŠ¶æ€ç å’Œé”™è¯¯ä¿¡æ¯
4. **æ— çŠ¶æ€**: æ¯æ¬¡è¯·æ±‚ç‹¬ç«‹ï¼Œä¸ä¿å­˜å†å²

## ğŸ†š `/api/chat` vs `/api/generate`

| ç‰¹æ€§ | `/api/chat` (ZenAI) | `/api/generate` (é€šç”¨) |
|------|---------------------|------------------------|
| **ç”¨é€”** | ç¦…æ„å¯¹è¯ | é€šç”¨LLMè°ƒç”¨ |
| **Prompt** | ZenAIç³»ç»ŸpromptåŒ…è£… | å®Œå…¨è‡ªå®šä¹‰ |
| **å“åº”** | ç¦…æ„é£æ ¼ | åŸå§‹LLMè¾“å‡º |
| **è®°å½•** | ä¿å­˜åˆ°æ•°æ®åº“ | ä¸ä¿å­˜ |
| **åé¦ˆ** | æ”¯æŒç”¨æˆ·åé¦ˆ | æ— åé¦ˆæœºåˆ¶ |
| **è¿›åŒ–** | å‚ä¸ç³»ç»Ÿè¿›åŒ– | ç‹¬ç«‹è¿è¡Œ |
| **é€‚åˆåœºæ™¯** | ZenHeartäº§å“å¯¹è¯ | è‡ªå®šä¹‰åº”ç”¨ |

## ğŸ“Š æ€§èƒ½å»ºè®®

1. **å¹¶å‘æ§åˆ¶**: åŒæ—¶è¯·æ±‚ä¸è¦è¶…è¿‡10ä¸ª
2. **Tokenæ§åˆ¶**: æ ¹æ®éœ€è¦è°ƒæ•´max_tokens
3. **é”™è¯¯é‡è¯•**: å»ºè®®å®ç°æŒ‡æ•°é€€é¿é‡è¯•
4. **è¶…æ—¶è®¾ç½®**: å»ºè®®è®¾ç½®30ç§’è¶…æ—¶

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ–°æ¥å£ï¼Ÿ
**A**: ZenAIçš„ `/api/chat` ä¸“ä¸ºç¦…æ„å¯¹è¯è®¾è®¡ï¼Œä¼šç”¨ç¦…æ„é£æ ¼åŒ…è£…å“åº”ã€‚ç¬¦å·ç‚¼é‡‘æœ¯ç­‰åº”ç”¨éœ€è¦ç›´æ¥çš„JSONè¾“å‡ºï¼Œå› æ­¤éœ€è¦é€šç”¨æ¥å£ã€‚

### Q2: è¿™ä¸ªæ¥å£ä¼šå½±å“ZenAIçš„è¿›åŒ–å—ï¼Ÿ
**A**: ä¸ä¼šã€‚`/api/generate` æ˜¯ç‹¬ç«‹çš„ï¼Œä¸è®°å½•äº¤äº’å†å²ï¼Œä¸å‚ä¸ç³»ç»Ÿè¿›åŒ–ã€‚

### Q3: å¯ä»¥ç”¨è¿™ä¸ªæ¥å£æ›¿ä»£ `/api/chat` å—ï¼Ÿ
**A**: æŠ€æœ¯ä¸Šå¯ä»¥ï¼Œä½†ä¸æ¨èã€‚`/api/chat` æœ‰å®Œæ•´çš„åé¦ˆå’Œè¿›åŒ–æœºåˆ¶ï¼Œæ˜¯ZenHeartäº§å“çš„æ ¸å¿ƒã€‚

### Q4: Temperatureè®¾ç½®å¤šå°‘åˆé€‚ï¼Ÿ
**A**: å¯¹äºç¬¦å·ç‚¼é‡‘æœ¯è¿™ç§éœ€è¦ä¸€å®šåˆ›æ„ä½†åˆè¦æ±‚æ ¼å¼æ­£ç¡®çš„åº”ç”¨ï¼Œæ¨è0.7ã€‚

## ğŸ”§ æ•…éšœæ’æŸ¥

### **é”™è¯¯ï¼š500 Internal Server Error**
```json
{
  "detail": "LLM generation failed: ..."
}
```
**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥LLMé…ç½®æ˜¯å¦æ­£ç¡®ï¼ˆ.envæ–‡ä»¶ï¼‰
2. æ£€æŸ¥API keyæ˜¯å¦æœ‰æ•ˆ
3. æ£€æŸ¥ç½‘ç»œè¿æ¥

### **é”™è¯¯ï¼š422 Validation Error**
```json
{
  "detail": [
    {
      "loc": ["body", "prompt"],
      "msg": "field required"
    }
  ]
}
```
**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿è¯·æ±‚åŒ…å«å¿…éœ€çš„ `prompt` å­—æ®µ

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [ZenAI APIå®Œæ•´æ–‡æ¡£](./README.md)
- [ç¬¦å·ç‚¼é‡‘æœ¯æ¸¸æˆæ–‡æ¡£](../../docs/symbol-alchemy-feature.md)
- [LLMé…ç½®è¯´æ˜](./token-management_v0.1.md)

---

*æœ€åæ›´æ–°: 2026-01-20*
*ç‰ˆæœ¬: v1.0.0*
